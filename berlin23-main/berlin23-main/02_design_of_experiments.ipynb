{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal design of experiments\n",
    "\n",
    "When unlabeled training data is abundant but obtaining the proper labels is\n",
    "expensive, you may want to select only a subset of the data to label and use for\n",
    "training your model. Sometimes, random choice may be good enough, but if the\n",
    "data is redundant or skewed towards certain categories, or when your budget is\n",
    "very low, then this might give bad results. *Design of Experiments* is the\n",
    "process of selecting a good subset of the available samples to query the label\n",
    "of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preamble\n",
    "\n",
    "Let's first import what we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "from os import environ\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import picos as pc\n",
    "from plotly import graph_objects as go\n",
    "from plotly.offline import init_notebook_mode\n",
    "from plotly.subplots import make_subplots\n",
    "from skimage.transform import downscale_local_mean\n",
    "\n",
    "LAYOUT = dict(\n",
    "    margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    "    paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    ")\n",
    "init_notebook_mode()\n",
    "\n",
    "# Detect if we are running on Binder; then we need to scale down problem sizes.\n",
    "BINDER = \"BINDER_SERVICE_HOST\" in environ\n",
    "if BINDER:\n",
    "    print(\"Running in a Binder instance. Problems will be scaled down.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will work with the\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) image data\n",
    "set, stored in the attached `fashion_mnist.npz`. Below we define one function to\n",
    "load the images into a data matrix of varying size and one to visualize such a\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_matrix(name, images, *, random_subset=None, downscale=None):\n",
    "    \"\"\"Convert a collection of 2D images to a PICOS data matrix.\n",
    "\n",
    "    The rows of the returned matrix are (a random subset of) the scaled down,\n",
    "    vectorized, and normalized input images.\n",
    "\n",
    "    :param images: A n×w×h tensor of n many w×h grayscale images.\n",
    "    :returns: An n'×(wh) data matrix, for n' ≤ n as specified.\n",
    "    \"\"\"\n",
    "    if random_subset:\n",
    "        if random_subset > len(images):\n",
    "            raise ValueError(\"Not enough data!\")\n",
    "\n",
    "        images = images[np.random.choice(range(len(images)), random_subset)]\n",
    "\n",
    "    if downscale:\n",
    "        images = np.array(\n",
    "            [downscale_local_mean(img, downscale) for img in images]\n",
    "        )\n",
    "\n",
    "    vectorized = np.reshape(images, (len(images), -1))\n",
    "    normalized = vectorized / np.linalg.norm(vectorized, axis=1)[:, np.newaxis]\n",
    "\n",
    "    return pc.Constant(name, normalized)\n",
    "\n",
    "\n",
    "def plot(A, cols=10):\n",
    "    \"\"\"Recover images from a PICOS data matrix and plot them.\"\"\"\n",
    "    A = A.np2d  # Convert to NumPy.\n",
    "    n, d = A.shape\n",
    "    s = int(d**0.5)\n",
    "    assert s**2 == d\n",
    "    rows = ceil(n / cols)\n",
    "    fig = make_subplots(rows=rows, cols=cols)\n",
    "    for i, a_i in enumerate(A):\n",
    "        row, col = i // cols + 1, i % cols + 1\n",
    "        gray_image = (a_i.reshape((s, s)) / np.max(a_i) * 255).astype(int)\n",
    "        rgb_image = gray_image[..., np.newaxis].repeat(3, axis=2)\n",
    "        fig.add_trace(go.Image(z=rgb_image), row=row, col=col)\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(showticklabels=False)\n",
    "    fig.update_layout(LAYOUT | dict(height=rows * 100))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Load training and testing images from the dataset (attached).\n",
    "dataset = np.load(\"fashion_mnist.npz\")\n",
    "A_img = dataset[\"train_images\"]\n",
    "T_img = dataset[\"test_images\"]\n",
    "\n",
    "# Plot some (normalized) images.\n",
    "A = data_matrix(\"A\", A_img, random_subset=40)\n",
    "plot(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Sample selection\n",
    "\n",
    "The data set that we loaded conveniently comes with both data points (the\n",
    "images) and associated labels (the type of fashion item depicted). This is not\n",
    "always the case — in practice we are confronted with an abundance of *unlabeled*\n",
    "data, which can be used in unsupervised learning tasks such as clustering but\n",
    "which is not suited for supervised tasks like classification. Producing the\n",
    "missing labels may be expensive, time consuming, error-prone, or even dangerous\n",
    "if it involves performing real world measurements or experiments. In such\n",
    "situations, we would like a **hint which data is the most valuable to put a\n",
    "label on**.\n",
    "\n",
    "The theory of **Optimal Design** (of experiments) provides statistical tools to\n",
    "tackle this problem that can often be implemented as convex programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Statistical background\n",
    "\n",
    "Under simplifying assumptions, we can give a definite answer to the question\n",
    "which samples are the best. In Optimal Design, one typically assumes a linear\n",
    "regression model of the form\n",
    "\n",
    "$$\n",
    "    y = A\\theta + \\epsilon\n",
    "$$\n",
    "\n",
    "where the rows $a_i$ of $A$ are unlabeled samples (describing parameters for\n",
    "experiments that one could perform), the entries $\\epsilon_i$ of the error\n",
    "vector $\\epsilon$ are uncorellated random variables with mean\n",
    "$\\mathbb{E}[\\epsilon_i] = 0$ and variance $\\mathbb{V}[\\epsilon_i] = \\sigma^2$,\n",
    "and where the entries $y_i$ of $y$ are (random) measurements that depend on both\n",
    "the experiment performed and on the measurement error $\\epsilon_i$. Finally,\n",
    "$\\theta$ is the parameter of the model that one would like to estimate.\n",
    "\n",
    "A well known estimator for the parameter $\\theta$ given experiments $A$ and\n",
    "measurements $y$ is the *ordinary least squares (OLS)* estimator\n",
    "\n",
    "$$\n",
    "    \\theta_\\mathrm{OLS} = (A^T A)^{-1} A^T y,\n",
    "$$\n",
    "\n",
    "which, given that $A$ has full column rank, is the unique optimum solution of\n",
    "the least-squares problem\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{minimize} ~&~ {\\lVert A\\theta - y \\rVert}^2. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The popularity of $\\theta_\\mathrm{OLS}$ is theoretically well motivated: it\n",
    "is the [*best linear unbiased estimator\n",
    "(BLUE)*](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) for the\n",
    "above setting, meaning that it is the estimator with the smallest variance among\n",
    "all estimators $\\hat{\\theta}$ that are unbiased (i.e., $\\mathbb{E}[\\hat{\\theta}]\n",
    "= \\theta$).\n",
    "\n",
    "When we speak of the *variance* of an estimator, we refer to the fact that the\n",
    "estimator is a random variable defined in terms of noisy measurements. Its\n",
    "covariance matrix is\n",
    "\n",
    "$$\n",
    "    \\mathbb{V}[\\theta_\\mathrm{OLS}] = \\Sigma_A = \\sigma^2 {(A^T A)}^{-1}.\n",
    "$$\n",
    "\n",
    "Note that **the \"smaller\" the matrix $\\Sigma_A$ is, the more certain we can be\n",
    "that $\\theta_\\mathrm{OLS}$ is a good estimate of the true parameter\n",
    "$\\theta$** (assuming that the linear model is an accurate description of\n",
    "reality). In Optimal Design, we thus want to design the matrix of experiments\n",
    "$A$ such that $\\Sigma_A$ becomes \"small\". The inverse of $\\Sigma_A$ is the\n",
    "**information matrix**\n",
    "\n",
    "$$\n",
    "    M_A = \\Sigma_A^{-1} = \\sigma^{-2} A^T A.\n",
    "$$\n",
    "\n",
    "Intuitively, a \"large\" matrix $M_A$ indicates that the data points stored in the\n",
    "rows of $A$ are very informative for our model. While we will stay in the realm\n",
    "of linear regression for this notebook, note that the information matrix can be\n",
    "defined more generally for nonlinear models, although then it will depend also\n",
    "on $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setting\n",
    "\n",
    "Above we have introduced the information matrix $M_A$ that describes the\n",
    "information content of the data in $A$ for fitting a linear model. Now, we will\n",
    "assume that $A$ has a large number of rows $a_i$ but we have only a small\n",
    "experimentation budget for producing the associated labels $y_i$. If we denote\n",
    "by $z_i$ the number of times that we perform the experiment $a_i$, then the\n",
    "information matrix becomes\n",
    "\n",
    "$$\n",
    "    M_A(z) = \\frac{1}{\\sigma^2} \\sum_{i = 1}^n z_i a_i^T a_i.\n",
    "$$\n",
    "\n",
    "The term $\\sigma^{-2}$ is just a constant that we may disregard in the following.\n",
    "The *exact* optimal design problem is then the combinatorial problem\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ \\phi(M_A(z)) \\\\\n",
    "    \\text{subject to} ~&~ \\mathbf{1}^T z \\leq \\beta, \\\\\n",
    "    \\text{where} ~&~ z \\in \\mathbb{Z}_{\\geq 0}^n. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here $\\beta \\in \\mathbb{Z}_{\\geq 0}$ is our experimentation budget and $\\phi$ is\n",
    "a concave function that turns the positive semidefinite matrix $M_A(z)$ into a\n",
    "scalar that we can maximize (such as $\\operatorname{tr}$, $\\log\\det$, or\n",
    "$\\lambda_{\\min}$).\n",
    "\n",
    "The exact optimal design problem is NP-hard in general. However, the problem\n",
    "becomes convex and tractable if we drop the requirement that $z$ is integral. We\n",
    "define the (relaxed) **optimal design problem** as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ \\phi(M_A(w)) \\tag{1 | $\\phi$-optimal design} \\\\\n",
    "    \\text{subject to} ~&~ \\mathbf{1}^T w = 1, \\\\\n",
    "    \\text{where} ~&~ w \\in \\mathbb{R}_{\\geq 0}^n \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here, $w$ represents a probability distribution (or importance assignment) over\n",
    "the rows of $A$. In machine learning terms, we can think of it as a vector of\n",
    "**sample weights**. The entries of $w_i^*$ that are zero in an optimal solution\n",
    "$w^*$ correspond to *inessential* samples: we have good reason to not spend much\n",
    "effort on labeling them.\n",
    "\n",
    "While we're at it, let's define a function to compute $M_A(w)$ from a data\n",
    "matrix $A$ and a vector variable $w$, for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_matrix(A, w):\n",
    "    \"\"\"Compute the information matrix M(w) for data A and sample weights w.\n",
    "\n",
    "    The constant factor of 1/σ² is omitted.\n",
    "\n",
    "    :param A: An n×d PICOS matrix containing n many d-dimensional samples.\n",
    "    :param w: A PICOS n-dimensional vector expression.\n",
    "    :returns: The d×d information matrix M(w) = ∑ᵢ wᵢ aᵢᵀ aᵢ (aᵢ i-th row of A).\n",
    "\n",
    "    This function is a fast replacement for:\n",
    "\n",
    "    >>> pc.sum(w[i] * (A[i, :].T * A[i, :]) for i in range(n)).renamed(\"M(w)\")\n",
    "    \"\"\"\n",
    "    n, d = A.shape\n",
    "    C = np.array([np.outer(vec, vec).ravel() for vec in A.np2d])\n",
    "    assert C.shape == (n, d**2)\n",
    "    return (w.T * C).reshaped((d, d)).renamed(f\"M({w.string})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we want a function that plots the images in $A$ corresponding to\n",
    "nonzero entries of $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_solution(A, w):\n",
    "    \"\"\"Display images corresponding to nonzero entries of sample weights w.\"\"\"\n",
    "    w = w if isinstance(w, np.ndarray) else w.np\n",
    "    support = np.where(w > 1e-6)[0]\n",
    "    print(f\"\\nDesign w: {len(support)}/{len(w)} samples selected.\")\n",
    "    plot(A[support, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting data for one category: Bayesian $c$-optimal design\n",
    "\n",
    "Different experimental designs (read: data points to label and associated\n",
    "training weights) are obtained for different choices of the scalarization\n",
    "function $\\phi$. The first design we want to study is the Bayesian $c$-optimal\n",
    "design, for a **row** vector $c \\in \\mathbb{R}^d$ and a parameter $\\lambda > 0$:\n",
    "\n",
    "$$\n",
    "   \\phi_{c,\\lambda}(X) = -c (X + \\lambda I)^{-1} c^T\n",
    "$$\n",
    "\n",
    "This criterion lets us identify data that provides information **best suited for\n",
    "estimating the label of the fixed data point $c$**. In our example, if $c$ is\n",
    "the image of a shoe, then we can expect the support of an optimal solution $w$\n",
    "to identify a variety of shoes in the dataset. The *Bayesian* in the name refers\n",
    "to a setup with prior knowledge on the parameter $\\theta$ and the scalar\n",
    "$\\lambda$ has a statistical interpretation in this context; see [Pilz\n",
    "(1991)](#References) for a monograph on Bayesian design. Here, however, we will\n",
    "use $\\lambda$ merely as a knob to control the number of data points selected.\n",
    "\n",
    "Since $X = M(w)$ is positive semidefinite by construction as a sum of outer\n",
    "products, we have that $X + \\lambda I$ is positive definite and so taking the\n",
    "inverse is both well-defined and a convex function. It follows that\n",
    "$\\phi_{c,\\lambda}(M(w))$ is a concave function of $w$, so *maximizing* it\n",
    "results indeed in a convex problem. For a long time, only a reformulation as an\n",
    "SDP was known, which can be difficult to solve in high dimensions. A more\n",
    "performant reformulation as an SOCP is due to Sagnol ([2011](#References)). More\n",
    "recently, a connection to the following regression problem was discovered\n",
    "([Sagnol and Pauwels, 2019](#References)):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\text{minimize} ~&~\n",
    "      \\lVert A^T x - c^T \\rVert^2 + \\lambda \\lVert x \\rVert_1^2\n",
    "      \\tag{2 | squared lasso} \\\\\n",
    "      \\text{where} ~&~ x \\in \\mathbb{R}^n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "When $\\lVert x \\rVert_1^2$ is replaced with $\\lVert x \\rVert_1$, then (2)\n",
    "becomes the well-known lasso (or $\\ell_1$-regularized) regression problem, in\n",
    "which one tries to estimate $x$ from $A$ and $c$ (assuming $A^Tx \\approx c^T$)\n",
    "while simultaneously nudging $x$ to be a sparse vector: the penalty $\\lVert x\n",
    "\\rVert_1 = \\sum_{i=1}^n |x_i|$ is large if $x$ has many nonzero entries. To\n",
    "compute a $c$-optimal design, we will make use of the following result (see\n",
    "[Sagnol and Pauwels (2019)](#References) or [Sagnol and Pronzato (t.\n",
    "a.)](#References) for a proof):\n",
    "\n",
    "Let $\\phi = \\phi_{c,\\lambda}$. Then,\n",
    "\n",
    "1. the optimal value of (2) equals the optimal value of (1) multiplied by\n",
    "   $\\lambda$ and,\n",
    "2. if $x$ solves (2), then $w$ with $w_i = \\frac{|x_i|}{\\lVert x\n",
    "   \\rVert_1}$ solves (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.1:**\n",
    "   \n",
    "   1. Implement the $\\phi$-optimal design problem for $\\phi =\n",
    "   \\phi_{c,\\lambda}$ using the connection to the squared lasso regression\n",
    "   problem described above.\n",
    "   2. Vary $\\lambda$. What do you observe?\n",
    "\n",
    "  <details>\n",
    "  <summary style='display: list-item'>PICOS and NumPy syntax (vector norms)</summary>\n",
    "\n",
    "  | on paper | in picos |\n",
    "  | --- | --- |\n",
    "  | ${\\lVert x \\rVert}_p$ | `pc.Norm(x, p)` |\n",
    "  | ${\\lVert x \\rVert}_2 = \\lVert x \\rVert$ | `pc.Norm(x, 2)` or `abs(x)` |\n",
    "  | $t^2$ | `t**2` |\n",
    "\n",
    "  For recovering the solution $w$, NumPy's entry-wise functions are useful:\n",
    "\n",
    "  | on paper | in numpy |\n",
    "  | --- | --- |\n",
    "  | $\\begin{pmatrix} \\|x_1\\| & \\cdots & \\|x_n\\| \\end{pmatrix}$ | `abs(x)` |\n",
    "  | ${\\lVert x \\rVert}_1$ | `sum(abs(x))` |\n",
    "\n",
    "  Recall that you can access the value of a PICOS expression `x` as a NumPy array\n",
    "  via `x.np`.\n",
    "\n",
    "  </details>\n",
    "\n",
    "  <details>\n",
    "  <summary style='display: list-item'>Recommended hint</summary>\n",
    "\n",
    "  - PICOS does not currently have a class to represent a squared $\\ell_1$-norm.\n",
    "    You will have to improvise with an auxiliary variable.\n",
    "\n",
    "  </details>\n",
    "\n",
    "  <details>\n",
    "  <summary style='display: list-item'>Hint</summary>\n",
    "\n",
    "  - Write ${\\lVert x \\rVert}_1^2$ as $\\tau^2$ with ${\\lVert x \\rVert}_1 \\leq\n",
    "    \\tau$.\n",
    "\n",
    "  </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can decrease NUM_DATA and/or increment DOWNSCALE if solution is too slow.\n",
    "NUM_DATA = 30 if BINDER else 300\n",
    "DOWNSCALE = 1\n",
    "\n",
    "# Load the data matrix from the training set at random.\n",
    "A = data_matrix(\"A\", A_img, random_subset=NUM_DATA, downscale=DOWNSCALE)\n",
    "n, d = A.shape\n",
    "\n",
    "# Load one reference image from the test set at random.\n",
    "c = data_matrix(\"c\", T_img, random_subset=1, downscale=DOWNSCALE)\n",
    "\n",
    "# Inspect the expressions and plot the reference image.\n",
    "print(repr(A), repr(c), sep=\"\\n\")\n",
    "plot(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rerun the above cell to get a new reference sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and solve the problem.\n",
    "P = pc.Problem(\"Bayesian c-optimality\")\n",
    "l = pc.Constant(\"λ\", 0.5)\n",
    "\n",
    "\n",
    "# TODO: Recover a solution for the design vector w.\n",
    "# w =\n",
    "\n",
    "# Show selected images.\n",
    "show_solution(A, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should see a handful of pieces of clothing that are mostly\n",
    "of the same kind as the reference piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting data for multiple categories: Bayesian L(inear)-optimal design\n",
    "\n",
    "So far we can extract data points whose labels are informative for predicting\n",
    "the label of *one* reference point. This may be useful when a certain category\n",
    "is underrepresented in a preliminary dataset and we are looking for additional\n",
    "data that is similar yet varied. On its own, though, a $c$-optimal design is not\n",
    "useful for training a classifier.\n",
    "\n",
    "An L-optimal (or $A_K$-optimal) design is the extension of a $c$-optimal design\n",
    "to **multiple reference vectors**, stored as the rows of a matrix $K \\in\n",
    "\\mathbb{R}^{r \\times d}$:\n",
    "\n",
    "$$\n",
    "   \\phi_{K,\\lambda}(X) = -K (X + \\lambda I)^{-1} K^T\n",
    "$$\n",
    "\n",
    "The intuition is similar as for $c$-optimality: We are looking for a design that\n",
    "jointly supports the (linear) estimation of all points in $K$. Again, $\\lambda$\n",
    "encodes prior knowledge on $\\theta$ and serves as a knob to control sparsity.\n",
    "\n",
    "By an extension of the earlier result, the $L$-optimality problem is equivalent\n",
    "to the following variant of the *group lasso* problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\text{minimize} ~&~\n",
    "      {\\lVert A^T X - K^T \\rVert}^2 + \\lambda {\\lVert X^T \\rVert}_{2,1}^2\n",
    "      \\tag{3 | squared group lasso} \\\\\n",
    "      \\text{where} ~&~ X \\in \\mathbb{R}^{n \\times r}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Here,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   \\lVert B \\rVert_{p,q}\n",
    "   =\n",
    "   {\\left(\n",
    "      \\sum_{j=1}^n\n",
    "      {\\left(\n",
    "         \\sum_{i=1}^m\n",
    "         {\\left(\n",
    "            |B_{i,j}|^p\n",
    "         \\right)}\n",
    "      \\right)}^{\\frac{q}{p}}\n",
    "   \\right)}^{\\frac{1}{q}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "denotes the matrix $L_{p,q}$-norm of a matrix $B \\in \\mathbb{R}^{m \\times n}$,\n",
    "which is the vector $\\ell_{q}$-norm of the vector $\\ell_{p}$-norms of the\n",
    "columns of $B$. In particular,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   {\\lVert X^T \\rVert}_{2,1}^2\n",
    "   =\n",
    "   \\sum_{i=1}^n \\sqrt{\\sum_{j=1}^r X_{i,j}^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "denotes the sum of the Euclidean norms of the rows of $X$.\n",
    "\n",
    "More precisely, the extended result states that for the L-optimality criterion\n",
    "$\\phi = \\phi_{K,\\lambda}$, we have that\n",
    "\n",
    "1. the optimal value of (3) equals the optimal value of (1) multiplied by\n",
    "   $\\lambda$ and,\n",
    "2. if $X$ solves (3), then $w$ with $w_i = \\frac{\\Vert x_i \\rVert}{\\lVert X\n",
    "   \\rVert_{1, 2}}$ solves (1), where $x_i$ is the $i$-th row of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.2:** Implement the $\\phi$-optimal design problem for $\\phi =\n",
    "   \\phi_{K,\\lambda}$ using its squared group lasso formulation.\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>PICOS and NumPy syntax (matrix norms)</summary>\n",
    "\n",
    "   | on paper | in picos | in numpy | note |\n",
    "   | --- | --- | --- | --- |\n",
    "   | ${\\lVert A \\rVert}_{p,q}$ | `pc.Norm(A, p, q)` | | see also the [docs for Norm](https://picos-api.gitlab.io/picos/api/picos.expressions.exp_norm.html#picos.expressions.exp_norm.Norm) |\n",
    "   | $\\begin{pmatrix} \\lVert a_1 \\rVert & \\cdots & \\lVert a_m \\rVert \\end{pmatrix}$  | | `np.linalg.norm(A, axis=1)` | $a_i$ is $i$-th row of $A$\n",
    "   | ${\\lVert A^T \\rVert}_{2,1}$ | | `sum(np.linalg.norm(A, axis=1))` | |\n",
    "\n",
    "   Again, NumPy entry-wise functions are useful for recovering a solution value\n",
    "   for $w$.\n",
    "\n",
    "   </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINDER:\n",
    "    NUM_REFERENCE = 2\n",
    "    NUM_DATA = 15\n",
    "    DOWNSCALE = 3\n",
    "    np.random.seed(4)  # Hand-pick a seed that solves quickly.\n",
    "else:\n",
    "    NUM_REFERENCE = 2\n",
    "    NUM_DATA = 150\n",
    "    DOWNSCALE = 2\n",
    "\n",
    "# Load the data.\n",
    "A = data_matrix(\"A\", A_img, random_subset=NUM_DATA, downscale=DOWNSCALE)\n",
    "K = data_matrix(\"K\", T_img, random_subset=NUM_REFERENCE, downscale=DOWNSCALE)\n",
    "(n, d), r = A.shape, NUM_REFERENCE\n",
    "\n",
    "print(repr(A), repr(K), sep=\"\\n\")\n",
    "plot(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may rerun until a fashionable outfit appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and solve the problem.\n",
    "P = pc.Problem(\"L-optimality\")\n",
    "l = pc.Constant(\"λ\", 0.5)\n",
    "\n",
    "\n",
    "# TODO: Recover the design vector w.\n",
    "# w =\n",
    "\n",
    "# Show selected images alongside the one encoded by c.\n",
    "show_solution(A, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, you should obtain a small collection that spans all reference samples well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised selection: E(igenvalue)-optimal design\n",
    "\n",
    "The above sample selection was semi-supervised: we had provided hand-picked\n",
    "reference samples to tell the procedure which type of training data we are\n",
    "interested in. Next, we want to study scalarizations of the information matrix\n",
    "that do not depend on such guidance.\n",
    "\n",
    "We also want to visualize how the selection is performed. To this end, we need\n",
    "low-dimensional data that we can plot. Let's use principal component analysis to\n",
    "project the images into three dimensions (`A_full` will contain the\n",
    "full-dimensional and `A` the projected data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINDER:\n",
    "    NUM_DATA = 50\n",
    "    np.random.seed(2)\n",
    "else:\n",
    "    NUM_DATA = 500\n",
    "\n",
    "# Project data to three dimensions using PCA.\n",
    "A_full = data_matrix(\"A_full\", A_img, random_subset=NUM_DATA)  # Load the data.\n",
    "s = int(A_full.shape[1] ** 0.5)  # Store image side length for plotting.\n",
    "A_full_np = A_full.np2d  # Convert the data matrix to NumPy.\n",
    "if BINDER:  # Binder struggles with eigenvalue computations.\n",
    "    A_full_np = np.array([downscale_local_mean(x, 4) for x in A_full_np])\n",
    "A_full_np -= np.mean(A_full_np, 0)  # Center it.\n",
    "S = np.cov(A_full_np, rowvar=False)  # Compute its covariance matrix S.\n",
    "v = np.linalg.eigh(S)[1]  # Compute the eigenvectors of S.\n",
    "v = v[:, : -(3 + 1) : -1]  # Select the three first principal eigenvectors.\n",
    "A_np = np.dot(A_full_np, v)  # Project the data onto the space spanned by them.\n",
    "A = pc.Constant(\"A\", A_np)  # Load the result again as a PICOS expression.\n",
    "\n",
    "print(repr(A_full), repr(A), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the projected data. You can hover over points to display the\n",
    "original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(trace, points, state):\n",
    "    global s, fw2\n",
    "    index = points.point_inds[0]\n",
    "    vector = A_full.np[index]\n",
    "    gray_image = (vector.reshape((s, s)) / np.max(vector) * 255).astype(int)\n",
    "    rgb_image = gray_image[..., np.newaxis].repeat(3, axis=2)\n",
    "    fw2.data[0].z = rgb_image\n",
    "\n",
    "\n",
    "x, y, z = A.np.T\n",
    "trace = go.Scatter3d(x=x, y=y, z=z, mode=\"markers\", marker=dict(size=3))\n",
    "fw1 = go.FigureWidget(data=trace, layout=LAYOUT)\n",
    "fw1.update_layout(scene=dict(aspectmode=\"data\"))\n",
    "fw2 = go.FigureWidget(data=go.Image(z=np.zeros((s, s, 3))))\n",
    "fw2.update_xaxes(title=\"selected image\")\n",
    "fw1.data[0].on_hover(show_image)\n",
    "widgets.HBox([fw1, fw2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above lasso reformulations, the fact that we were trying to maximize a\n",
    "scalar criterion function of the information matrix $M_A(w)$ was rather\n",
    "implicit. This will now change as we dive into some of the classic criteria,\n",
    "starting with **E-optimality**. Here, we aim to **maximize the smallest eigenvalue\n",
    "of the information matrix**,\n",
    "\n",
    "$$\n",
    "   \\phi_\\text{E}(X) = \\lambda_{\\min}(X).\n",
    "$$\n",
    "\n",
    "For the linear regression setting described in the *Statistical background*\n",
    "section, this is equivalent to minimizing the largest eigenvalue (aka the\n",
    "spectral radius) of the estimator covariance matrix $\\Sigma$.\n",
    "\n",
    "Let us restate the problem we are looking to solve:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ \\lambda_{\\min}(M_A(w)) \\tag{4 | E-optimal design} \\\\\n",
    "    \\text{subject to} ~&~ \\mathbf{1}^T w = 1 \\\\\n",
    "    \\text{where} ~&~ w \\in \\mathbb{R}_{\\geq 0}^n,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for\n",
    "\n",
    "$$\n",
    "    M_A(w) = \\sum_{i = 1}^n w_i a_i^T a_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.3:** Implement problem (4) for the low-dimensional data matrix `A`.\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>PICOS syntax (matrix inequalities)</summary>\n",
    "\n",
    "  | on paper | in picos |\n",
    "  | --- | --- |\n",
    "  | $A \\preceq B$ | `A << B` |\n",
    "  | $A \\succeq B$ | `A >> B` |\n",
    "  | $I_n$ | `pc.I(n)` |\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint</summary>\n",
    "\n",
    "   - Recall from the introduction talk that for $X \\in \\mathbb{S}^n$, it is\n",
    "     $\\lambda_{\\min}(X) \\leq \\tau \\Longleftrightarrow X \\succeq \\tau I_n$.\n",
    "\n",
    "   </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a variable w and the information matrix M(w) for A.\n",
    "# NOTE: You can use the function information_matrix defined earlier.\n",
    "n, d = A.shape\n",
    "# w =\n",
    "# M =\n",
    "\n",
    "# TODO: Implement and solve the problem.\n",
    "P = pc.Problem(\"E-optimality\")\n",
    "\n",
    "\n",
    "# Display the solution in terms of the full-dimensional data.\n",
    "show_solution(A_full, w)\n",
    "\n",
    "# Save the solution for later.\n",
    "M_E = ~M  # The ~ operator converts the current value to a constant expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a small number of items that span three or more distinct categories.\n",
    "\n",
    "Should you stumble upon an eigenvalue problem again, there is a shortcut in PICOS:\n",
    "\n",
    "<details>\n",
    "<summary style='display: list-item'>Spoiler</summary>\n",
    "\n",
    "For a symmetric or hermitian matrix $A$:\n",
    "\n",
    "| on paper | in picos | note |\n",
    "| --- | --- | --- |\n",
    "| $\\lambda_{\\text{min}}(A)$ | `pc.lambda_min(A)` | is a concave function of $A$, so you can maximize it\n",
    "| $\\lambda_{\\text{max}}(A)$ | `pc.lambda_max(A)` | is a convex function of $A$, so you can minimize it\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Geometric interpretation & Duality\n",
    "\n",
    "We will next shed some light on how the above selection is made using a duality\n",
    "argument. To this end, consider the **Lagrange dual problem** of problem (4):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{minimize} ~&~ \\nu \\tag{5} \\\\\n",
    "    \\text{subject to}\n",
    "        ~&~ \\operatorname{tr}(\\Lambda) = 1, \\\\\n",
    "        ~&~ a_i \\Lambda a_i^T \\leq \\nu & \\forall i \\in [n], \\\\\n",
    "    \\text{where}\n",
    "        ~&~ \\Lambda \\succeq 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The dual problem has one constraint for each variable and one (non-negative)\n",
    "variable for each (in-)equality constraint of the primal problem.\n",
    "\n",
    "<details>\n",
    "<summary style='display: list-item'>Details</summary>\n",
    "\n",
    "- The dual variable $\\nu \\in \\mathbb{R}$ corresponds to the primal constraint\n",
    "  $\\mathbf{1}^T w = 1$,\n",
    "- the dual variable $\\Lambda \\in \\mathbb{S}^d$ with $\\Lambda \\succeq 0$ corresponds to the linear\n",
    "  matrix inequality that you introduced for the eigenvalue bound,\n",
    "- the dual constraint $\\operatorname{tr}(\\Lambda) = 1$ corresponds to the primal\n",
    "  auxiliary variable that you introduced for the epigraph reformulation, and\n",
    "- the dual constraint $a_i \\Lambda a_i^T \\leq \\nu$ corresponds to $w_i$ for every $i \\in [n]$.\n",
    "\n",
    "</details>\n",
    "\n",
    "By **strong duality**, problems (4) and (5) have the same optimal value, that is\n",
    "$\\nu = \\lambda_{\\min}(M_A(w))$ for a primal optimal $w$ and dual optimal $\\nu$\n",
    "and $\\Lambda$. Informally, this means that formulation (5) is another way of\n",
    "thinking about problem (4).\n",
    "\n",
    "Problem (5) is *almost* useful for better understanding the E-optimal design. We\n",
    "need to rearrange it a bit. First, observe that $\\nu \\geq 0$ in any feasible\n",
    "solution (since $\\Lambda \\succeq 0$) and further $\\nu \\neq 0$ when $A$ has full\n",
    "column rank (since $\\Lambda \\neq 0$). Dividing all constraints by $\\nu$ and\n",
    "performing the change of variable $X = \\frac{1}{\\nu} \\Lambda$ gives us the\n",
    "equivalent dual problem\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{minimize} ~&~ \\nu \\tag{5b} \\\\\n",
    "    \\text{subject to}\n",
    "        ~&~ \\operatorname{tr}(X) = \\frac{1}{\\nu}, \\\\\n",
    "        ~&~ a_i X a_i^T \\leq 1 & \\forall i \\in [n], \\\\\n",
    "    \\text{where}\n",
    "        ~&~ X \\succeq 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary style='display: list-item'>Alternative formulation</summary>\n",
    "\n",
    "As $\\nu$ only denotes the inverse of the trace that we want to minimize, you may\n",
    "find the dual written in the literature (e.g. in Boyd and Vandenberghe\n",
    "([2014, problem 7.30](#References))) more compactly as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{maximize} ~&~ \\operatorname{tr}(X) \\tag{5c} \\\\\n",
    "    \\text{subject to}\n",
    "        ~&~ a_i X a_i^T \\leq 1 & \\forall i \\in [n], \\\\\n",
    "    \\text{where}\n",
    "        ~&~ X \\succeq 0.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Observe that (5b) and (5c) have the same solution (value of $X$ at optimality)\n",
    "but not the same optimal value, so (5c) is not strictly a dual of problem (4).\n",
    "However, only (5c) is a convex problem because of the nonlinear equality\n",
    "constraint in (5b).\n",
    "\n",
    "</details>\n",
    "\n",
    "We will next make use of the fact that the dual constraint $a_i X a_i^T \\leq 1$\n",
    "corresponds to the $i$-th entry of the primal variable $w$, which encodes the\n",
    "E-optimal design. By the **complementary slackness** condition — another\n",
    "statement relating primal and dual problem — we have for every $i \\in [n]$ that\n",
    "\n",
    "$$\n",
    "    w_i (a_i X a_i^T - 1) = 0\n",
    "$$\n",
    "\n",
    "holds whenever $w$ and $(X, \\nu)$ form an optimal solution pair for problems (4)\n",
    "and (5b). In other words, **only those data points $a_i$ can be part of the\n",
    "design (i.e., $w_i > 0$) for which $a_i X a_i^T = 1$ for an optimal dual\n",
    "solution $X$.** Geometrically, this means that all selected $a_i$ lie on the\n",
    "hull of the ellipsoid $\\mathcal{E} = \\{x \\in \\mathbb{R}^d \\mid x^T X x \\leq\n",
    "1\\}$!\n",
    "\n",
    "The next task is to obtain an optimal dual solution $X$ for plotting the\n",
    "ellipsoid $\\mathcal{E}$. There are two options to do so: we could implement the\n",
    "dual problem and solve it separately or we could make use of the fact that PICOS\n",
    "provides us with a dual solution as a byproduct of solving the primal: the\n",
    "optimal values of dual variables are stored in the `dual` properties of the\n",
    "corresponding primal constraint objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Task 2.4 / Option 1:**\n",
    "\n",
    "   1. Implement and solve the dual problem (5c) (see *Alternative formulation*) to obtain a dual optimal solution $X$.\n",
    "\n",
    "   **Option 2:**\n",
    "\n",
    "   1. Edit your implementation of problem (4) above: store the linear matrix inequality constraint in a Python variable `lmi` before adding it to the problem.\n",
    "   2. Obtain a dual solution for $\\Lambda$ in problem (5) from `lmi.dual`. Use it to compute a solution $X$ for problem (5b).\n",
    "\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>PICOS syntax for Option 1</summary>\n",
    "\n",
    "   | on paper | in picos | note |\n",
    "   | --- | --- | --- |\n",
    "   | $X \\in \\mathbb{S}^d$ | `X = pc.SymmetricVariable(\"X\", d)` | |\n",
    "   | $A_{i,j}$ | `A[i,j]` | entry in $i$-th row and $j$-th column of $A$ |\n",
    "   | $A_{i,\\cdot} = a_i$ | `A[i,:]` | $i$-th row of $A$ as a row vector |\n",
    "   | $A_{\\cdot,j}$ | `A[:,j]` | $j$-th column of $A$ as a column vector |\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Adding constraint families</summary>\n",
    "\n",
    "   You can write, e.g.,\n",
    "\n",
    "   ```\n",
    "   problem += constraint1, constraint2\n",
    "   ```\n",
    "\n",
    "   to add multiple constraints at once. You may also supply them as a list. In particular, you may use Python [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) to define a family of constraints (here $x_i^2 \\leq y_i \\; \\forall i \\in [n]$):\n",
    "\n",
    "   ```\n",
    "   problem += [x[i]**2 <= y[i] for i in range(n)]\n",
    "   ```\n",
    "\n",
    "   If you supply constraints as a list, PICOS tries to guess a compact string representation for the family. Try it out by printing the dual problem!\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Obtaining dual values</summary>\n",
    "\n",
    "   Instead of, say,\n",
    "   \n",
    "   ```\n",
    "   problem += A*x <= b \n",
    "   ```\n",
    "\n",
    "   you can write\n",
    "\n",
    "   ```\n",
    "   constraint = A*x <= b \n",
    "   problem += constraint\n",
    "   ```\n",
    "\n",
    "   and, after solving `problem`, access the dual value associated with the constraint via\n",
    "\n",
    "   ```\n",
    "   x = constraint.dual\n",
    "   ```\n",
    "\n",
    "   Note that `dual` returns values as a CVXOPT matrix. These are normally interchangeable with NumPy matrices. In case of doubt, you can write:\n",
    "\n",
    "   ```\n",
    "   x = np.array(constraint.dual)\n",
    "   ```\n",
    "\n",
    "   </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Obtain an optimal dual solution X.\n",
    "\n",
    "\n",
    "\n",
    "def unit_ball(*, res=20):\n",
    "    \"\"\"Generate points on the 3D unit sphere.\"\"\"\n",
    "    u, v = np.mgrid[0 : 2 * np.pi : res * 2j, 0 : np.pi : res * 1j]\n",
    "    u, v = u.ravel(), v.ravel()\n",
    "    return np.stack((np.cos(u) * np.sin(v), np.sin(u) * np.sin(v), np.cos(v)))\n",
    "\n",
    "\n",
    "def ellipsoid(X, *, res=20):\n",
    "    \"\"\"Generate points on an ellipsoid given by a quadratic form.\"\"\"\n",
    "    L = np.linalg.cholesky(np.linalg.inv(X))\n",
    "    return L @ unit_ball(res=res)\n",
    "\n",
    "\n",
    "# Collect samples supported by w in a submatrix S of A.\n",
    "support = np.where(w.np > 1e-6)[0]\n",
    "S = A[support, :]\n",
    "\n",
    "# Plot the ellipsoid given by the dual solution X.\n",
    "x, y, z = A.np.T\n",
    "sx, sy, sz = S.np.T\n",
    "ex, ey, ez = ellipsoid(X)\n",
    "trace1 = go.Scatter3d(x=x, y=y, z=z, name=\"data\", mode=\"markers\", marker_size=3)\n",
    "trace2 = go.Scatter3d(\n",
    "    x=sx, y=sy, z=sz, name=\"selected\", mode=\"markers\", marker_size=7\n",
    ")\n",
    "kwargs = dict(alphahull=0, opacity=0.3, showlegend=True, hoverinfo=\"skip\")\n",
    "trace3 = go.Mesh3d(x=ex, y=ey, z=ez, name=\"tight dual constraint\", **kwargs)\n",
    "go.Figure(\n",
    "    data=[trace1, trace2, trace3],\n",
    "    layout=dict(\n",
    "        title_text=\"E-optimal design visualized\",\n",
    "        margin=dict(t=50, b=0, l=0, r=0),\n",
    "        height=600,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure should explain why in three dimensions we get only a couple of\n",
    "categories and why possibly some of them are represented by multiple samples.\n",
    "Also worth noting: optimal designs are not robust to outliers. Imagine what\n",
    "would happen to the ellipsoid if there was a point far outside the cloud!\n",
    "\n",
    "(If $X$ has eigenvalues close to zero, then the ellipsoid will be extremely\n",
    "stretched. If this happens, re-running the notebook should give you another\n",
    "instance that's more pleasant to look at.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: A(verage)-optimal design\n",
    "\n",
    "Another classic is the **A-optimal** design, which maximizes the criterion\n",
    "\n",
    "$$\n",
    "   \\phi_{\\text{A}}(X) = -\\operatorname{tr}(X^{-1}).\n",
    "$$\n",
    "\n",
    "This is a special case of the L-optimal design that we computed earlier: it is\n",
    "$\\phi_{\\text{A}} = \\phi_{K,\\lambda}$ for $K = I$ and $\\lambda = 0$.\n",
    "Unfortunately, the group lasso reformulation we had used does not extend to the\n",
    "limit case of $\\lambda = 0$. So we will need another trick to get rid of the\n",
    "matrix inverse.\n",
    "\n",
    "For brevity we may minimize $-\\phi_{\\text{A}}(M_A(w))$ instead of maximizing\n",
    "$\\phi_{\\text{A}}(M_A(w))$, which is equivalent up to the sign of the objective\n",
    "value. We then arrive at the following problem formulation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{minimize} ~&~ \\operatorname{tr}({M_A(w)}^{-1}) \\tag{6 | A-optimal design} \\\\\n",
    "    \\text{subject to} ~&~ \\mathbf{1}^T w = 1 \\\\\n",
    "    \\text{where} ~&~ w \\in \\mathbb{R}_{\\geq 0}^n.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To get rid of the inverse, we can use a staple lemma in conic optimization, the\n",
    "**Schur complement**:\n",
    "\n",
    "For symmetric matrices $P$ and $R$ with $R \\succ 0$, it is\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        P & Q^T \\\\\n",
    "        Q & R\n",
    "    \\end{bmatrix} \\succeq 0\n",
    "    \\quad\\Longleftrightarrow\\quad\n",
    "    P - Q^T R^{-1} Q \\succeq 0.\n",
    "$$\n",
    "\n",
    "You may assume that ${M_A(w)}^{-1} = \\Sigma_A(w) \\succ 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Bonus task 2.5:**\n",
    "   \n",
    "   1. Use the Schur complement lemma to reformulate problem (6) as a semidefinite program.\n",
    "   2. Implement the reformulated problem.\n",
    "   \n",
    "   <details>\n",
    "   <summary style='display: list-item'>PICOS syntax</summary>\n",
    "\n",
    "   | on paper | in picos |\n",
    "   | --- | --- |\n",
    "   | $X \\in \\mathbb{S}^d$ | `X = pc.SymmetricVariable(\"X\", d)` |\n",
    "   | $\\operatorname{tr}(A)$ | `pc.trace(A)` or `A.tr` |\n",
    "   | $$\\begin{bmatrix}A & B\\end{bmatrix}$$ | `(A & B)` |\n",
    "   | $$\\begin{bmatrix}A \\\\ B\\end{bmatrix}$$ | `(A // B)` |\n",
    "   | $$\\begin{bmatrix}A & B \\\\ C & D\\end{bmatrix}$$ | `pc.block([[A, B], [C, D]])` |\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint relating the trace and matrix inequalities</summary>\n",
    "\n",
    "   - For symmetric $A$ and $B$, it is $$A \\succeq B \\implies \\operatorname{tr}(A) \\geq\n",
    "\\operatorname{tr}(B).$$ Otherwise, $\\operatorname{tr}(A) - \\operatorname{tr}(B) =\n",
    "\\operatorname{tr}(A - B) < 0$ would imply that $A - B$ has a negative eigenvalue,\n",
    "contradicting $A - B \\succeq 0$.\n",
    "\n",
    "   - Thus, a first step could be to introduce an auxiliary symmetric matrix variable whose trace we minimize instead, and an associated linear matrix inequality.\n",
    "\n",
    "   </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and solve problem (6) for M(w) defined earlier.\n",
    "P = pc.Problem(\"A-optimality\")\n",
    "\n",
    "\n",
    "# Display the solution in terms of the full-dimensional data.\n",
    "show_solution(A_full, w)\n",
    "\n",
    "# Save the solution for later.\n",
    "M_A = ~M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing E- and A-optimal designs\n",
    "\n",
    "There is another way to visualize optimal designs, which also involves\n",
    "ellipsoids.\n",
    "\n",
    "Recall the statistical motivation for maximizing the information matrix: this\n",
    "amounts to minimizing the covariance matrix $\\Sigma(w) = {M^{-1}(w)}$ of the\n",
    "least-squares estimator $\\theta_{\\text{OLS}}$, assuming a linear regression\n",
    "setting. The E-optimal design maximizes $\\lambda_{\\min}(M(w))$ and thereby\n",
    "minimizes $\\lambda_{\\max}(\\Sigma(w))$ while the A-optimal design explicitly\n",
    "minimizes $\\operatorname{tr}(\\Sigma(w))$. For a design $w$ and a given\n",
    "confidence level $\\alpha$, the **$\\alpha$-confidence region for the true\n",
    "parameter $\\theta$** is the ellipsoid\n",
    "\n",
    "$$\n",
    "    \\mathcal{E} = \\{\\theta \\mid (\\theta - \\theta_{\\text{OLS}})^T \\Sigma^{-1}(w)\n",
    "    (\\theta - \\theta_{\\text{OLS}}) \\leq \\beta\\},\n",
    "$$\n",
    "\n",
    "where $\\beta$ depends on $\\alpha$ and on the data dimension $d$. Note that this\n",
    "ellipsoid lives in the parameter space of the linear regression model — unlike\n",
    "the ellipsoid from Task 2.4 that lived in the feature space.\n",
    "\n",
    "Setting $\\beta = 1$ and $\\theta_{\\text{OLS}} = 0$, we can compare the shape of\n",
    "this ellipsoid for different designs $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Bonus task 2.6:**\n",
    "\n",
    "   1. Complete the code below to display confidence regions for different designs.\n",
    "   2. What property of the confidence ellipsoid is optimized by the E- and A-optimal design, respectively?\n",
    "   3. Why are the axes of the uniform design's confidence region aligned with the coordinate axes?\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>PICOS syntax</summary>\n",
    "\n",
    "   | on paper | in picos | note |\n",
    "   | --- | --- | --- |\n",
    "   | $J_{m, n}$ | `pc.J(m, n)` | all-ones matrix |\n",
    "   | $\\mathbf{1}_n$ | `pc.J(n)` | all-ones vector |\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint on (2)</summary>\n",
    "\n",
    "   - For $\\beta = 1$ and $\\theta_{\\text{OLS}} = 0$, the ellipsoid $\\mathcal{E}$ is the result of applying the linear transformation $\\Sigma^{\\frac{1}{2}}$ to the Euclidean unit ball $\\mathcal{B} = \\{x \\mid {\\lVert x \\rVert}^2 = 1\\}$. What do the eigenvalues of $\\Sigma^{-1}$ tell you about this transformation?\n",
    "   - For a symmetric matrix, the trace equals the sum of the eigenvalues.\n",
    "\n",
    "   </details>\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Hint on (3)</summary>\n",
    "\n",
    "   - Recall that we used [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) for dimensionality reduction. What does this mean for the covariance matrix of the transformed data?\n",
    "\n",
    "   </details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a constant PICOS vector representing a uniform design, which uses\n",
    "#       all available samples with the same small probability.\n",
    "# w_uniform =\n",
    "\n",
    "# Check that the design is a probability vector.\n",
    "assert np.all(w_uniform.np >= 0)\n",
    "assert np.allclose(w_uniform.sum.np, 1)\n",
    "\n",
    "# TODO: Compute the associated information matrix.\n",
    "# M_uniform =\n",
    "\n",
    "# Plot confidence ellipsoids for both designs.\n",
    "ux, uy, uz = ellipsoid(M_uniform.np)\n",
    "ex, ey, ez = ellipsoid(M_E.np)\n",
    "ax, ay, az = ellipsoid(M_A.np)\n",
    "trace_theta_ols = go.Scatter3d(\n",
    "    x=[0], y=[0], z=[0], name=\"least-squares estimator\", mode=\"markers\"\n",
    ")\n",
    "kwargs = dict(alphahull=0, opacity=0.3, showlegend=True)\n",
    "trace_u = go.Mesh3d(x=ux, y=uy, z=uz, name=\"uniform design\", **kwargs)\n",
    "trace_e = go.Mesh3d(x=ex, y=ey, z=ez, name=\"E-optimal design\", **kwargs)\n",
    "trace_a = go.Mesh3d(x=ax, y=ay, z=az, name=\"A-optimal design\", **kwargs)\n",
    "go.Figure(\n",
    "    data=[trace_theta_ols, trace_u, trace_e, trace_a],\n",
    "    layout=dict(\n",
    "        title_text=\"Confidence regions for the true parameter\",\n",
    "        margin=dict(t=50, b=0, l=0, r=0),\n",
    "        height=600,\n",
    "        scene=dict(\n",
    "            xaxis=dict(showticklabels=False),\n",
    "            yaxis=dict(showticklabels=False),\n",
    "            zaxis=dict(showticklabels=False),\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence region for the uniform design roughly corresponds to picking data\n",
    "samples to label at random. It should appear larger than the two optimal\n",
    "designs, which in turn should have a slightly different shape.\n",
    "\n",
    "(You can click on legend entries to disable the corresponding trace.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proving the Schur complement lemma\n",
    "\n",
    "The Schur complement is among the most useful lemmas in conic optimization.\n",
    "Its proof is not too difficult. Let's recover it!\n",
    "\n",
    "For this, we let $P \\in \\mathbb{S}^p$ and $R \\in \\mathbb{S}^r$ with $R \\succ 0$\n",
    "and define $A \\coloneqq \\begin{bmatrix} P & Q^T \\\\ Q & R \\end{bmatrix}$ for\n",
    "brevity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "   **Extra bonus task 2.7:** Prove the Schur complement lemma by filling in the gaps:\n",
    "\n",
    "   <details>\n",
    "   <summary style='display: list-item'>Detailed instructions</summary>\n",
    "\n",
    "   - (a) Use a definition of $A \\succeq 0$.\n",
    "   - (b) Partition $z$ into $u$ and $v$ and insert the definition of $A$.\n",
    "     Rewrite the scalar $u^T Q^T v$ as its transpose $v^T Q u$ to simplify a\n",
    "     bit.\n",
    "   - (c) Copy and paste, then think about why this makes sense. :-)\n",
    "   - (d) Use the following: For $u$ and $R \\succ 0$ fixed, the convex quadratic\n",
    "     problem $\\text{minimize}~v^T R v + 2 v^T Q u$ has the unique optimum\n",
    "     solution $v^* = -R^{-1} Q u$.\n",
    "   - (e) Simplify.\n",
    "   - (f) Confirm that the definition of positive semidefiniteness applies.\n",
    "\n",
    "   </details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div hidden>\n",
    "\n",
    "$$\n",
    "\\providecommand{\\TODO}{}\\renewcommand{\\TODO}{{\\color{red}{[\\ldots]}}}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    & & A &\\succeq 0 \\\\\n",
    "    &\\overset{\\text{(a)}}\\Longleftrightarrow &\n",
    "        \\forall z \\in \\mathbb{R}^{p + r} \\colon\n",
    "        \\TODO &\\geq 0 \\\\\n",
    "    &\\overset{\\text{(b)}}\\Longleftrightarrow &\n",
    "        \\forall (u, v) \\in \\mathbb{R}^p \\times \\mathbb{R}^r \\colon\n",
    "        \\TODO &\\geq 0 \\\\\n",
    "    &\\overset{\\text{(c)}}\\Longleftrightarrow & \n",
    "        \\forall u \\in \\mathbb{R}^p \\colon\n",
    "        \\inf_{v \\in \\mathbb{R}^r} \\left( \\TODO \\right) &\\geq 0 \\\\\n",
    "    &\\overset{\\text{(d)}}\\Longleftrightarrow &\n",
    "        \\forall u \\in \\mathbb{R}^p \\colon\n",
    "        \\TODO &\\geq 0 \\\\\n",
    "    &\\overset{\\text{(e)}}\\Longleftrightarrow &\n",
    "        \\forall u \\in \\mathbb{R}^p \\colon\n",
    "        u^T \\left( \\TODO \\right) u &\\geq 0 \\\\\n",
    "    &\\overset{\\text{(f)}}\\Longleftrightarrow &\n",
    "        P - Q^T R^{-1} Q &\\succeq 0.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "- A number of implementations for the quadratic (group) lasso problem, including\n",
    "  one in PICOS, is found at https://gitlab.com/gsagnol/qlasso.\n",
    "- There has been a [chain of\n",
    "  research](https://www.zib.de/projects/optimal-design-experiments) on\n",
    "  experimental design here at the ZIB.\n",
    "\n",
    "## References\n",
    "\n",
    "- Stephen Boyd and Lieven Vandenberghe. *Convex Optimization*. Cambridge\n",
    "  University Press, Cambridge, 2004. (online version:\n",
    "  https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)\n",
    "- Jürgen Pilz. *Bayesian Estimation and Experimental Design in Linear Regression\n",
    "  Models*. Wiley, New York, 1991.\n",
    "- Guillaume Sagnol, 2011. Computing optimal designs of multiresponse experiments\n",
    "  reduces to second-order cone programming. *Journal of Statistical Planning and\n",
    "  Inference*, 141, 1684-1708.\n",
    "  [doi:10.1016/j.jspi.2010.11.031](https://doi.org/10.1016/j.jspi.2010.11.031)\n",
    "  (preprint: [arXiv:0912.5467](https://arxiv.org/abs/0912.5467))\n",
    "- Guillaume Sagnol and Edouard Pauwels, 2019. An unexpected connection between\n",
    "  Bayes A-optimal designs and the Group Lasso. *Statistical Papers* 60, 565-584.\n",
    "  [doi:10.1007/s00362-018-01062-y](https://doi.org/10.1007/s00362-018-01062-y)\n",
    "  (preprint: [arXiv:1809.01931](https://arxiv.org/abs/1809.01931))\n",
    "- Guillaume Sagnol and Luc Pronzato, to appear. Fast screening rules for optimal\n",
    "  design via quadratic lasso reformulation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
